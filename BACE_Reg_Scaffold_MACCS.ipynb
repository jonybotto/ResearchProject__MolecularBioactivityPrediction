{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af04fbf",
   "metadata": {},
   "source": [
    "# ML Conventional Methods for Bioactivity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28442ee4",
   "metadata": {},
   "source": [
    "## BACE - Regression | Scaffold Splitting | MACCS Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3931e9",
   "metadata": {},
   "source": [
    "##### Import libraries\n",
    "\n",
    "- Helper function: load, split dataset, generate fingerprint\n",
    "\n",
    "- Load model from scikit-learn, torch\n",
    "\n",
    "- Load hyperopt module for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706621d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper.load_dataset import load_bace_regression\n",
    "from helper.preprocess import split_train_valid_test\n",
    "from helper.features_ml import smi_maccs\n",
    "from helper.cal_metrics import regression_metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from hyperopt import hp, tpe, fmin, Trials, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b4f1993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13bf588a370>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718e7",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a60163",
   "metadata": {},
   "source": [
    "##### Scaffold Splitting - BACE Regression Task\n",
    "\n",
    "- Load, split dataset\n",
    "\n",
    "- Generate fingerprint, defined target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6479585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "bace_reg = load_bace_regression()\n",
    "\n",
    "# Split dataset\n",
    "train, valid, test = split_train_valid_test(bace_reg, type='scaffold')\n",
    "merge = pd.concat((train, valid))\n",
    "\n",
    "# Generate fingerprint\n",
    "train_smis = train['SMILES']\n",
    "valid_smis = valid['SMILES']\n",
    "test_smis = test['SMILES']\n",
    "merge_smis = merge['SMILES']\n",
    "X_train = [smi_maccs(smi) for smi in train_smis]\n",
    "X_valid = [smi_maccs(smi) for smi in valid_smis]\n",
    "X_test = [smi_maccs(smi) for smi in test_smis]\n",
    "X_merge = [smi_maccs(smi) for smi in merge_smis]\n",
    "\n",
    "\n",
    "# Target defined\n",
    "y_train = train['pIC50']\n",
    "y_valid = valid['pIC50']\n",
    "y_test = test['pIC50']\n",
    "y_merge = merge['pIC50']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccac410",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning and model training\n",
    "\n",
    "- Pipeline: Hyperparameter tuning using valid set -> Train best params on train + valid -> Test on test set\n",
    "\n",
    "- Models to try:\n",
    "\n",
    "    - Support Vector Machine\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "    - Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15e97d",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8d1ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.31trial/s, best loss: 0.7727228031838892]\n",
      "\n",
      "SVM Results:\n",
      "\n",
      "Best params: {'C': 3.9096709154738605, 'epsilon': 0.10293754722055581, 'kernel': 'rbf'}\n",
      "\n",
      "+------------+---------+--------+\n",
      "| Metrics    |   Train |   Test |\n",
      "+============+=========+========+\n",
      "| RMSE       |  0.5131 | 1.2635 |\n",
      "+------------+---------+--------+\n",
      "| R2         |  0.8461 | 0.3876 |\n",
      "+------------+---------+--------+\n",
      "| MAPE       |  0.0556 | 0.148  |\n",
      "+------------+---------+--------+\n",
      "| Pearson R  |  0.921  | 0.6978 |\n",
      "+------------+---------+--------+\n",
      "| Spearman R |  0.9088 | 0.6708 |\n",
      "+------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "svm_search_space = {\n",
    "    'C': hp.loguniform('C', np.log(0.1), np.log(10)),\n",
    "    'epsilon': hp.uniform('epsilon', 0.01, 0.2),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly'])\n",
    "}\n",
    "\n",
    "def svm_objective(params):\n",
    "    model = SVR(\n",
    "        C=params['C'],\n",
    "        epsilon=params['epsilon'], \n",
    "        kernel=params['kernel']\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_valid_hat)\n",
    "    return mse\n",
    "\n",
    "best_svm_params = fmin(\n",
    "    fn=svm_objective,\n",
    "    space=svm_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_svm_params = space_eval(svm_search_space, best_svm_params)\n",
    "\n",
    "model = SVR(**best_svm_params)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = regression_metrics(y_merge, y_train_pred)\n",
    "test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"RMSE\", f'{train_metrics['rmse']:.4f}', f'{test_metrics['rmse']:.4f}'],\n",
    "    [\"R2\", f'{train_metrics['r2']:.4f}', f'{test_metrics['r2']:.4f}'],\n",
    "    [\"MAPE\", f'{train_metrics['mape']:.4f}', f'{test_metrics['mape']:.4f}'],\n",
    "    [\"Pearson R\", f'{train_metrics['pearsonr']:.4f}', f'{test_metrics['pearsonr']:.4f}'],\n",
    "    [\"Spearman R\", f'{train_metrics['spearmanr']:.4f}', f'{test_metrics['spearmanr']:.4f}'],\n",
    "]\n",
    "\n",
    "print('\\nSVM Results:\\n')\n",
    "print(f'Best params: {best_svm_params}\\n')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_reg_scaffold_maccs_svm.txt', 'w') as file:\n",
    "    file.write('BACE regression | Scaffold Splitting | MACCS Features | SVM Model\\n\\n')\n",
    "    file.write('Results:\\n\\n')\n",
    "    file.write(f'Best params: {best_svm_params}\\n\\n')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa508a9",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d3c9086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.88trial/s, best loss: 0.8489026759291795]\n",
      "\n",
      "RF Results:\n",
      "\n",
      "Best params: {'max_depth': 18, 'max_features': 65, 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 15}\n",
      "\n",
      "+------------+---------+--------+\n",
      "| Metrics    |   Train |   Test |\n",
      "+============+=========+========+\n",
      "| RMSE       |  0.4823 | 1.2249 |\n",
      "+------------+---------+--------+\n",
      "| R2         |  0.864  | 0.4245 |\n",
      "+------------+---------+--------+\n",
      "| MAPE       |  0.0602 | 0.1574 |\n",
      "+------------+---------+--------+\n",
      "| Pearson R  |  0.9337 | 0.7101 |\n",
      "+------------+---------+--------+\n",
      "| Spearman R |  0.924  | 0.7002 |\n",
      "+------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "n_feats = len(X_train[0])\n",
    "trials = Trials()\n",
    "\n",
    "rf_search_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 100, 5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 2, 20, 1)),\n",
    "    'max_features': scope.int(hp.quniform('max_features', 5, int(n_feats/2), 5)),\n",
    "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 2, 20, 1)),\n",
    "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 20, 1))\n",
    "}\n",
    "\n",
    "def rf_objective(params):\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth'], \n",
    "        max_features=params['max_features'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        random_state=SEED,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_valid_hat)\n",
    "    return mse\n",
    "\n",
    "best_rf_params = fmin(\n",
    "    fn=rf_objective,\n",
    "    space=rf_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_rf_params = space_eval(rf_search_space, best_rf_params)\n",
    "\n",
    "model = RandomForestRegressor(**best_rf_params, random_state=SEED)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = regression_metrics(y_merge, y_train_pred)\n",
    "test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"RMSE\", f'{train_metrics['rmse']:.4f}', f'{test_metrics['rmse']:.4f}'],\n",
    "    [\"R2\", f'{train_metrics['r2']:.4f}', f'{test_metrics['r2']:.4f}'],\n",
    "    [\"MAPE\", f'{train_metrics['mape']:.4f}', f'{test_metrics['mape']:.4f}'],\n",
    "    [\"Pearson R\", f'{train_metrics['pearsonr']:.4f}', f'{test_metrics['pearsonr']:.4f}'],\n",
    "    [\"Spearman R\", f'{train_metrics['spearmanr']:.4f}', f'{test_metrics['spearmanr']:.4f}'],\n",
    "]\n",
    "\n",
    "print('\\nRF Results:\\n')\n",
    "print(f'Best params: {best_rf_params}\\n')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_reg_scaffold_maccs_rf.txt', 'w') as file:\n",
    "    file.write('BACE regression | Scaffold Splitting | MACCS Features | RF Model\\n\\n')\n",
    "    file.write('Results:\\n\\n')\n",
    "    file.write(f'Best params: {best_rf_params}\\n\\n')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fa474",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb31dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.10trial/s, best loss: 0.8070644411415061]\n",
      "\n",
      "XGBoost Results:\n",
      "\n",
      "Best params: {'booster': 'gbtree', 'colsample_bytree': 0.6442453508893451, 'gamma': 0.028131677474597244, 'learning_rate': 0.11524607771929213, 'max_depth': 10, 'min_child_weight': 2, 'n_estimators': 290, 'reg_alpha': 0.0022397266188939175, 'reg_lambda': 15.054237280051655, 'subsample': 0.4930356104753837, 'tree_method': 'hist'}\n",
      "\n",
      "+------------+---------+--------+\n",
      "| Metrics    |   Train |   Test |\n",
      "+============+=========+========+\n",
      "| RMSE       |  0.3298 | 1.1392 |\n",
      "+------------+---------+--------+\n",
      "| R2         |  0.9364 | 0.5022 |\n",
      "+------------+---------+--------+\n",
      "| MAPE       |  0.0358 | 0.1378 |\n",
      "+------------+---------+--------+\n",
      "| Pearson R  |  0.968  | 0.7575 |\n",
      "+------------+---------+--------+\n",
      "| Spearman R |  0.9616 | 0.7214 |\n",
      "+------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "xgb_search_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 300, 5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 10, 1)),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.00001), np.log(100)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.0001), np.log(1.)),\n",
    "    \"booster\": hp.choice('booster', ['gbtree', 'gblinear']),\n",
    "    'tree_method': 'hist',\n",
    "    'gamma': hp.uniform('gamma', 0., 5.)\n",
    "}\n",
    "\n",
    "def xgb_objective(params):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_child_weight=params['min_child_weight'], \n",
    "        subsample=params['subsample'], \n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        booster=params['booster'],\n",
    "        tree_method=params['tree_method'],\n",
    "        gamma=params['gamma'],\n",
    "        random_state=SEED,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_valid_hat)\n",
    "    return mse\n",
    "\n",
    "best_xgb_params = fmin(\n",
    "    fn=xgb_objective,\n",
    "    space=xgb_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_xgb_params = space_eval(xgb_search_space, best_xgb_params)\n",
    "\n",
    "model = XGBRegressor(**best_xgb_params, random_state=SEED)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = regression_metrics(y_merge, y_train_pred)\n",
    "test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"RMSE\", f'{train_metrics['rmse']:.4f}', f'{test_metrics['rmse']:.4f}'],\n",
    "    [\"R2\", f'{train_metrics['r2']:.4f}', f'{test_metrics['r2']:.4f}'],\n",
    "    [\"MAPE\", f'{train_metrics['mape']:.4f}', f'{test_metrics['mape']:.4f}'],\n",
    "    [\"Pearson R\", f'{train_metrics['pearsonr']:.4f}', f'{test_metrics['pearsonr']:.4f}'],\n",
    "    [\"Spearman R\", f'{train_metrics['spearmanr']:.4f}', f'{test_metrics['spearmanr']:.4f}'],\n",
    "]\n",
    "\n",
    "\n",
    "print('\\nXGBoost Results:\\n')\n",
    "print(f'Best params: {best_xgb_params}\\n')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_reg_scaffold_maccs_xgb.txt', 'w') as file:\n",
    "    file.write('BACE regression | Scaffold Splitting | MACCS Features | XGBoost Model\\n\\n')\n",
    "    file.write('Results:\\n\\n')\n",
    "    file.write(f'Best params: {best_xgb_params}\\n\\n')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77815648",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b55b829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:47<00:00,  2.09trial/s, best loss: 0.5664690928444037]\n",
      "\n",
      "ANN Results:\n",
      "\n",
      "Best params: {'activation': 'gelu', 'batch_size': 48, 'dropout_rate': 0.47364860504708284, 'epochs': 100, 'hidden_dim_1': 64, 'hidden_dim_2': 192, 'hidden_dim_3': 96, 'hidden_dim_4': 160, 'learning_rate': 0.0028412138796748824, 'n_layers': 4, 'patience': 10}\n",
      "\n",
      "+------------+---------+--------+\n",
      "| Metrics    |   Train |   Test |\n",
      "+============+=========+========+\n",
      "| RMSE       |  0.9079 | 1.2522 |\n",
      "+------------+---------+--------+\n",
      "| R2         |  0.5181 | 0.3985 |\n",
      "+------------+---------+--------+\n",
      "| MAPE       |  0.1237 | 0.1709 |\n",
      "+------------+---------+--------+\n",
      "| Pearson R  |  0.7523 | 0.6654 |\n",
      "+------------+---------+--------+\n",
      "| Spearman R |  0.7106 | 0.6383 |\n",
      "+------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "class FCNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            hidden_dim_1=128,\n",
    "            hidden_dim_2=128,\n",
    "            hidden_dim_3=128,\n",
    "            hidden_dim_4=128,\n",
    "            dropout_rate=0.2,\n",
    "            activation='relu',\n",
    "            n_layers=4\n",
    "    ):\n",
    "        super(FCNClassifier, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2, bias=True)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3, bias=True)\n",
    "        self.fc4 = nn.Linear(hidden_dim_3, hidden_dim_4, bias=True)\n",
    "        \n",
    "        output_dims = [hidden_dim_1, hidden_dim_2, hidden_dim_3, hidden_dim_4]\n",
    "        last_hidden_dim = output_dims[n_layers-1]\n",
    "        self.out = nn.Linear(last_hidden_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)                     # Layer 1\n",
    "        x = self._activation(x)\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        if self.n_layers >= 2:              # Layer 2 (if applicable)\n",
    "            x = self.fc2(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        if self.n_layers >= 3:              # Layer 3 (if applicable)\n",
    "            x = self.fc3(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if self.n_layers >= 4:              # Layer 4 (if applicable)\n",
    "            x = self.fc4(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'gelu':\n",
    "            return F.gelu(x)\n",
    "        elif self.activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        elif self.activation == 'selu':\n",
    "            return F.selu(x)\n",
    "        else:\n",
    "            return F.relu(x)\n",
    "    \n",
    "def train_ann(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        learning_rate = 0.001,\n",
    "        batch_size=128,\n",
    "        epochs=100,\n",
    "        patience=10,\n",
    "        device='cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.as_tensor(np.asarray(X_train), dtype=torch.float32),\n",
    "        torch.as_tensor(np.asarray(y_train), dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    early_stopping = True if X_valid is not None and y_valid is not None else False\n",
    "    best_mse = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    best_state = None\n",
    "    best_num_epochs = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Valid\n",
    "        if early_stopping:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
    "                y_valid_pred = model(X_valid_tensor).cpu().numpy().flatten()\n",
    "                mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                no_improvement_count = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                best_num_epochs += 1\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= patience:\n",
    "                    break\n",
    "    \n",
    "    if early_stopping and best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        return model, best_num_epochs\n",
    "        \n",
    "    return model, epochs\n",
    "\n",
    "def predict_test(model, X, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        predictions = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def ann_objective(\n",
    "        params,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        input_dim,\n",
    "        device='cpu'\n",
    "):\n",
    "    model = FCNClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim_1=params['hidden_dim_1'],\n",
    "        hidden_dim_2=params['hidden_dim_2'],\n",
    "        hidden_dim_3=params['hidden_dim_3'],\n",
    "        hidden_dim_4=params['hidden_dim_4'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        activation=params['activation'],\n",
    "        n_layers=params['n_layers']\n",
    "    )\n",
    "\n",
    "    model, best_num_epoch = train_ann(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        epochs=params['epochs'],\n",
    "        patience=params['patience'],\n",
    "        device=device,\n",
    "        batch_size=params['batch_size']\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
    "        y_valid_pred = model(X_valid_tensor).cpu().numpy().flatten()\n",
    "        mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "    return {\n",
    "        'loss': mse,\n",
    "        'status': 'ok',\n",
    "        'best_num_epoch': best_num_epoch\n",
    "    }\n",
    "\n",
    "ann_search_space = {\n",
    "    'n_layers': scope.int(hp.quniform('n_layers', 1, 4, 1)),\n",
    "    'hidden_dim_1': scope.int(hp.quniform('hidden_dim_1', 32, 192, 32)),\n",
    "    'hidden_dim_2': scope.int(hp.quniform('hidden_dim_2', 32, 192, 32)),\n",
    "    'hidden_dim_3': scope.int(hp.quniform('hidden_dim_3', 32, 192, 32)),\n",
    "    'hidden_dim_4': scope.int(hp.quniform('hidden_dim_4', 32, 192, 32)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'batch_size': scope.int(hp.quniform('batch_size', 32, 128, 16)),\n",
    "    'epochs': 100,\n",
    "    'patience': 10,\n",
    "    'activation': hp.choice('activation', ['relu', 'selu', 'elu', 'gelu'])\n",
    "}\n",
    "\n",
    "input_dim = len(X_train[0])\n",
    "\n",
    "def objective_fn(params):\n",
    "    return ann_objective(\n",
    "        params=params,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_valid=X_valid,\n",
    "        y_valid=y_valid,\n",
    "        input_dim=input_dim\n",
    "    )\n",
    "\n",
    "best_ann_params = fmin(\n",
    "    fn=objective_fn,\n",
    "    space=ann_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_ann_params = space_eval(ann_search_space, best_ann_params)\n",
    "\n",
    "model = FCNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim_1=best_ann_params['hidden_dim_1'],\n",
    "    hidden_dim_2=best_ann_params['hidden_dim_2'],\n",
    "    hidden_dim_3=best_ann_params['hidden_dim_3'],\n",
    "    hidden_dim_4=best_ann_params['hidden_dim_4'],\n",
    "    dropout_rate=best_ann_params['dropout_rate'],\n",
    "    activation=best_ann_params['activation'],\n",
    "    n_layers=best_ann_params['n_layers']\n",
    ")\n",
    "\n",
    "best_trial = trials.best_trial\n",
    "best_num_epochs = best_trial['result']['best_num_epoch']\n",
    "\n",
    "model, _ = train_ann(\n",
    "    model,\n",
    "    X_merge,\n",
    "    y_merge,\n",
    "    X_valid=None,\n",
    "    y_valid=None,\n",
    "    learning_rate=best_ann_params['learning_rate'],\n",
    "    batch_size=best_ann_params['batch_size'],\n",
    "    epochs=best_num_epochs,\n",
    ")\n",
    "\n",
    "y_train_pred = predict_test(model, X_merge)\n",
    "y_test_pred = predict_test(model, X_test)\n",
    "\n",
    "train_metrics = regression_metrics(y_merge, y_train_pred)\n",
    "test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"RMSE\", f'{train_metrics['rmse']:.4f}', f'{test_metrics['rmse']:.4f}'],\n",
    "    [\"R2\", f'{train_metrics['r2']:.4f}', f'{test_metrics['r2']:.4f}'],\n",
    "    [\"MAPE\", f'{train_metrics['mape']:.4f}', f'{test_metrics['mape']:.4f}'],\n",
    "    [\"Pearson R\", f'{train_metrics['pearsonr']:.4f}', f'{test_metrics['pearsonr']:.4f}'],\n",
    "    [\"Spearman R\", f'{train_metrics['spearmanr']:.4f}', f'{test_metrics['spearmanr']:.4f}'],\n",
    "]\n",
    "\n",
    "print('\\nANN Results:\\n')\n",
    "print(f'Best params: {best_ann_params}\\n')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_reg_scaffold_maccs_ann.txt', 'w') as file:\n",
    "    file.write('BACE regression | Scaffold Splitting | MACCS Features | ANN Model\\n\\n')\n",
    "    file.write('Results:\\n\\n')\n",
    "    file.write(f'Best params: {best_ann_params}\\n\\n')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ebcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.13.0_venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
