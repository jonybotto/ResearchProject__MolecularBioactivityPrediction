{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65aae650",
   "metadata": {},
   "source": [
    "# ML Conventional Methods for Bioactivity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314720e",
   "metadata": {},
   "source": [
    "## BACE - Classification | Scaffold Splitting | MACCS Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3931e9",
   "metadata": {},
   "source": [
    "##### Import libraries\n",
    "\n",
    "- Helper function: load, split dataset, generate fingerprint\n",
    "\n",
    "- Load model from scikit-learn, torch\n",
    "\n",
    "- Load hyperopt module for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706621d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper.load_dataset import load_bace_classification\n",
    "from helper.preprocess import split_train_valid_test\n",
    "from helper.features_ml import smi_maccs\n",
    "from helper.cal_metrics import classification_metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from hyperopt import hp, tpe, fmin, Trials, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb68567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2276a3ea370>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718e7",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a60163",
   "metadata": {},
   "source": [
    "##### Scaffold Splitting - BACE Classification Task\n",
    "\n",
    "- Load, split dataset\n",
    "\n",
    "- Generate fingerprint, defined target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6479585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "bace_class = load_bace_classification()\n",
    "\n",
    "# Split dataset\n",
    "train, valid, test = split_train_valid_test(bace_class, type='scaffold')\n",
    "merge = pd.concat((train, valid))\n",
    "\n",
    "# Generate fingerprint\n",
    "train_smis = train['SMILES']\n",
    "valid_smis = valid['SMILES']\n",
    "test_smis = test['SMILES']\n",
    "merge_smis = merge['SMILES']\n",
    "X_train = [smi_maccs(smi) for smi in train_smis]\n",
    "X_valid = [smi_maccs(smi) for smi in valid_smis]\n",
    "X_test = [smi_maccs(smi) for smi in test_smis]\n",
    "X_merge = [smi_maccs(smi) for smi in merge_smis]\n",
    "\n",
    "\n",
    "# Target defined\n",
    "y_train = train['Class']\n",
    "y_valid = valid['Class']\n",
    "y_test = test['Class']\n",
    "y_merge = merge['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccac410",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning and model training\n",
    "\n",
    "- Pipeline: Hyperparameter tuning using valid set -> Train best params on train + valid -> Test on test set\n",
    "\n",
    "- Models to try:\n",
    "\n",
    "    - Support Vector Machine\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "    - Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15e97d",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8d1ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 23.59trial/s, best loss: -0.7837837837837838]\n",
      "\n",
      "SVM Results:\n",
      "\n",
      "Best params: {'C': 0.6448662503199115, 'kernel': 'poly'}\n",
      "\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8685  | 0.6776 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8685  | 0.6776 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.8735  | 0.9718 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.8623  | 0.4198 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8688  | 0.7811 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.8865  | 0.5948 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.8470  | 0.9444 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9321  | 0.8093 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.9078  | 0.8476 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "svm_search_space = {\n",
    "    'C': hp.loguniform('C', np.log(0.1), np.log(10)),\n",
    "    # 'epsilon': hp.uniform('epsilon', 0.01, 0.2),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly'])\n",
    "}\n",
    "\n",
    "def svm_objective(params):\n",
    "    model = SVC(\n",
    "        C=params['C'],\n",
    "        kernel=params['kernel'],\n",
    "        random_state=SEED\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_valid_hat)\n",
    "    return -f1\n",
    "\n",
    "best_svm_params = fmin(\n",
    "    fn=svm_objective,\n",
    "    space=svm_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_svm_params = space_eval(svm_search_space, best_svm_params)\n",
    "\n",
    "model = SVC(**best_svm_params, probability=True, random_state=SEED)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_train_score = model.predict_proba(X_merge)[:, 1]\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_score = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = classification_metrics(y_merge, y_train_pred, y_train_score)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred, y_test_score)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "print('\\nSVM Results:\\n')\n",
    "print(f'Best params: {best_svm_params}\\n')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_scaffold_maccs_svm.txt', 'w') as file:\n",
    "    file.write('BACE classification | Scaffold Splitting | MACCS Features | SVM Model\\n\\n')\n",
    "    file.write('Results:\\n\\n')\n",
    "    file.write(f'Best params: {best_svm_params}\\n\\n')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa508a9",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3c9086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.67trial/s, best loss: -0.7657657657657657]\n",
      "\n",
      "RF Results:\n",
      "\n",
      "Best params: {'max_depth': 20, 'max_features': 55, 'min_samples_leaf': 2, 'min_samples_split': 8, 'n_estimators': 40}\n",
      "\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.9118  | 0.6711 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.9118  | 0.6711 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.9121  | 0.9577 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.9115  | 0.4198 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.9121  | 0.7659 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.9269  | 0.5913 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.8939  | 0.9189 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9748  | 0.8148 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.9681  | 0.8283 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "n_feats = len(X_train[0])\n",
    "trials = Trials()\n",
    "\n",
    "rf_search_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 100, 5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 2, 20, 1)),\n",
    "    'max_features': scope.int(hp.quniform('max_features', 5, int(n_feats/2), 5)),\n",
    "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 2, 20, 1)),\n",
    "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 20, 1))\n",
    "}\n",
    "\n",
    "def rf_objective(params):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth'], \n",
    "        max_features=params['max_features'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        random_state=SEED,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_valid_hat)\n",
    "    return -f1\n",
    "\n",
    "best_rf_params = fmin(\n",
    "    fn=rf_objective,\n",
    "    space=rf_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_rf_params = space_eval(rf_search_space, best_rf_params)\n",
    "\n",
    "model = RandomForestClassifier(**best_rf_params, random_state=SEED)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_train_score = model.predict_proba(X_merge)[:, 1]\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_score = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = classification_metrics(y_merge, y_train_pred, y_train_score)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred, y_test_score)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "print('\\nRF Results:\\n')\n",
    "print(f'Best params: {best_rf_params}\\n')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_scaffold_maccs_rf.txt', 'w') as file:\n",
    "    file.write('BACE classification | Scaffold Splitting | MACCS Features | RF Model\\n\\n')\n",
    "    file.write('Results:\\n\\n')\n",
    "    file.write(f'Best params: {best_rf_params}\\n\\n')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fa474",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb31dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.42trial/s, best loss: -0.7476635514018691]\n",
      "\n",
      "XGBoost Results:\n",
      "\n",
      "Best params: {'booster': 'gbtree', 'colsample_bytree': 0.40230957861188427, 'gamma': 1.7577303773478095, 'learning_rate': 0.5945091351116877, 'max_depth': 9, 'min_child_weight': 2, 'n_estimators': 60, 'reg_alpha': 0.006884678253406707, 'reg_lambda': 0.00012181130295506245, 'subsample': 0.7591560656349016, 'tree_method': 'hist'}\n",
      "\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8949  | 0.6579 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8949  | 0.6579 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.8935  | 0.8732 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.8967  | 0.4691 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8955  | 0.7067 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.9142  | 0.5905 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.8724  | 0.8085 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9622  | 0.7698 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.9521  | 0.7893 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "xgb_search_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 300, 5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 10, 1)),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.00001), np.log(100)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.0001), np.log(1.)),\n",
    "    \"booster\": hp.choice('booster', ['gbtree', 'gblinear']),\n",
    "    'tree_method': 'hist',\n",
    "    'gamma': hp.uniform('gamma', 0., 5.)\n",
    "}\n",
    "\n",
    "def xgb_objective(params):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_child_weight=params['min_child_weight'], \n",
    "        subsample=params['subsample'], \n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        booster=params['booster'],\n",
    "        tree_method=params['tree_method'],\n",
    "        gamma=params['gamma'],\n",
    "        random_state=SEED,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_valid_hat)\n",
    "    return -f1\n",
    "\n",
    "best_xgb_params = fmin(\n",
    "    fn=xgb_objective,\n",
    "    space=xgb_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_xgb_params = space_eval(xgb_search_space, best_xgb_params)\n",
    "\n",
    "model = XGBClassifier(**best_xgb_params, random_state=SEED)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_train_score = model.predict_proba(X_merge)[:, 1]\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_score = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = classification_metrics(y_merge, y_train_pred, y_train_score)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred, y_test_score)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "print('\\nXGBoost Results:\\n')\n",
    "print(f'Best params: {best_xgb_params}\\n')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_scaffold_maccs_xgb.txt', 'w') as file:\n",
    "    file.write('BACE classification | Scaffold Splitting | MACCS Features | XGBoost Model\\n\\n')\n",
    "    file.write('Results:\\n\\n')\n",
    "    file.write(f'Best params: {best_xgb_params}\\n\\n')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77815648",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b55b829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:16<00:00,  1.30trial/s, best loss: -0.8200836820083682]\n",
      "\n",
      "ANN Results:\n",
      "\n",
      "Best params: {'activation': 'relu', 'batch_size': 112, 'dropout_rate': 0.2169016734007006, 'epochs': 100, 'hidden_dim_1': 160, 'hidden_dim_2': 96, 'hidden_dim_3': 192, 'hidden_dim_4': 160, 'learning_rate': 0.0026800644950214776, 'n_layers': 2, 'patience': 10}\n",
      "\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8281  | 0.6711 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8281  | 0.6711 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.7510  | 0.7887 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.9230  | 0.5679 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8458  | 0.6893 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.9231  | 0.6154 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.7507  | 0.7541 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9217  | 0.7913 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.8957  | 0.8203 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "class FCNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            hidden_dim_1=128,\n",
    "            hidden_dim_2=128,\n",
    "            hidden_dim_3=128,\n",
    "            hidden_dim_4=128,\n",
    "            dropout_rate=0.2,\n",
    "            activation='relu',\n",
    "            n_layers=4\n",
    "    ):\n",
    "        super(FCNClassifier, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2, bias=True)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3, bias=True)\n",
    "        self.fc4 = nn.Linear(hidden_dim_3, hidden_dim_4, bias=True)\n",
    "        \n",
    "        output_dims = [hidden_dim_1, hidden_dim_2, hidden_dim_3, hidden_dim_4]\n",
    "        last_hidden_dim = output_dims[n_layers-1]\n",
    "        self.out = nn.Linear(last_hidden_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)                     # Layer 1\n",
    "        x = self._activation(x)\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        if self.n_layers >= 2:              # Layer 2 (if applicable)\n",
    "            x = self.fc2(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        if self.n_layers >= 3:              # Layer 3 (if applicable)\n",
    "            x = self.fc3(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if self.n_layers >= 4:              # Layer 4 (if applicable)\n",
    "            x = self.fc4(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'gelu':\n",
    "            return F.gelu(x)\n",
    "        elif self.activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        elif self.activation == 'selu':\n",
    "            return F.selu(x)\n",
    "        else:\n",
    "            return F.relu(x)\n",
    "    \n",
    "def train_ann(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        learning_rate = 0.001,\n",
    "        batch_size=128,\n",
    "        epochs=100,\n",
    "        patience=10,\n",
    "        device='cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.as_tensor(np.asarray(X_train), dtype=torch.float32),\n",
    "        torch.as_tensor(np.asarray(y_train), dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    early_stopping = True if X_valid is not None and y_valid is not None else False\n",
    "    best_f1 = 0\n",
    "    no_improvement_count = 0\n",
    "    best_state = None\n",
    "    best_num_epochs = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Valid\n",
    "        if early_stopping:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
    "                y_valid_out = model(X_valid_tensor).cpu().numpy().flatten()\n",
    "                y_valid_pred = (y_valid_out > 0.5).astype(int)\n",
    "                f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                no_improvement_count = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                best_num_epochs = epoch + 1\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= patience:\n",
    "                    break\n",
    "    \n",
    "    if early_stopping and best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        return model, best_num_epochs\n",
    "        \n",
    "    return model, epochs\n",
    "\n",
    "def predict_test(model, X, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        predictions = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def ann_objective(\n",
    "        params,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        input_dim,\n",
    "        device='cpu'\n",
    "):\n",
    "    model = FCNClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim_1=params['hidden_dim_1'],\n",
    "        hidden_dim_2=params['hidden_dim_2'],\n",
    "        hidden_dim_3=params['hidden_dim_3'],\n",
    "        hidden_dim_4=params['hidden_dim_4'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        activation=params['activation'],\n",
    "        n_layers=params['n_layers']\n",
    "    )\n",
    "\n",
    "    model, best_num_epoch = train_ann(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        epochs=params['epochs'],\n",
    "        patience=params['patience'],\n",
    "        device=device,\n",
    "        batch_size=params['batch_size']\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
    "        y_valid_out = model(X_valid_tensor).cpu().numpy().flatten()\n",
    "        y_valid_pred = (y_valid_out > 0.5).astype(int)\n",
    "        f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "    return {\n",
    "        'loss': -f1,\n",
    "        'status': 'ok',\n",
    "        'best_num_epoch': best_num_epoch\n",
    "    }\n",
    "\n",
    "ann_search_space = {\n",
    "    'n_layers': scope.int(hp.quniform('n_layers', 1, 4, 1)),\n",
    "    'hidden_dim_1': scope.int(hp.quniform('hidden_dim_1', 32, 192, 32)),\n",
    "    'hidden_dim_2': scope.int(hp.quniform('hidden_dim_2', 32, 192, 32)),\n",
    "    'hidden_dim_3': scope.int(hp.quniform('hidden_dim_3', 32, 192, 32)),\n",
    "    'hidden_dim_4': scope.int(hp.quniform('hidden_dim_4', 32, 192, 32)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'batch_size': scope.int(hp.quniform('batch_size', 32, 128, 16)),\n",
    "    'epochs': 100,\n",
    "    'patience': 10,\n",
    "    'activation': hp.choice('activation', ['relu', 'selu', 'elu', 'gelu'])\n",
    "}\n",
    "\n",
    "input_dim = len(X_train[0])\n",
    "\n",
    "def objective_fn(params):\n",
    "    return ann_objective(\n",
    "        params=params,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_valid=X_valid,\n",
    "        y_valid=y_valid,\n",
    "        input_dim=input_dim\n",
    "    )\n",
    "\n",
    "best_ann_params = fmin(\n",
    "    fn=objective_fn,\n",
    "    space=ann_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_ann_params = space_eval(ann_search_space, best_ann_params)\n",
    "\n",
    "model = FCNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim_1=best_ann_params['hidden_dim_1'],\n",
    "    hidden_dim_2=best_ann_params['hidden_dim_2'],\n",
    "    hidden_dim_3=best_ann_params['hidden_dim_3'],\n",
    "    hidden_dim_4=best_ann_params['hidden_dim_4'],\n",
    "    dropout_rate=best_ann_params['dropout_rate'],\n",
    "    activation=best_ann_params['activation'],\n",
    "    n_layers=best_ann_params['n_layers']\n",
    ")\n",
    "\n",
    "best_trial = trials.best_trial\n",
    "best_num_epochs = best_trial['result']['best_num_epoch']\n",
    "\n",
    "model, _ = train_ann(\n",
    "    model,\n",
    "    X_merge,\n",
    "    y_merge,\n",
    "    X_valid=None,\n",
    "    y_valid=None,\n",
    "    learning_rate=best_ann_params['learning_rate'],\n",
    "    batch_size=best_ann_params['batch_size'],\n",
    "    epochs=best_num_epochs,\n",
    ")\n",
    "\n",
    "y_train_score = predict_test(model, X_merge)\n",
    "y_train_pred = (y_train_score > 0.5).astype(int)\n",
    "\n",
    "y_test_score = predict_test(model, X_test)\n",
    "y_test_pred = (y_test_score > 0.5).astype(int)\n",
    "\n",
    "train_metrics = classification_metrics(y_merge, y_train_pred, y_train_score)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred, y_test_score)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "print('\\nANN Results:\\n')\n",
    "print(f'Best params: {best_ann_params}\\n')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_scaffold_maccs_ann.txt', 'w') as file:\n",
    "    file.write('BACE classification | Scaffold Splitting | MACCS Features | ANN Model\\n\\n')\n",
    "    file.write('Results:\\n\\n')\n",
    "    file.write(f'Best params: {best_ann_params}\\n\\n')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde0687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.13.0_venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
