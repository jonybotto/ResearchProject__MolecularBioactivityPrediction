{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65aae650",
   "metadata": {},
   "source": [
    "## Extended Reduced Graph fingerprint for bioactivity predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3931e9",
   "metadata": {},
   "source": [
    "##### Import libraries\n",
    "\n",
    "- Helper function: load, split dataset, generate fingerprint\n",
    "\n",
    "- Load model from scikit-learn, torch\n",
    "\n",
    "- Load hyperopt module for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d85d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/Documents/AI4H/RTDS/Project/Coding/.venv/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv, global_max_pool, global_mean_pool, global_add_pool\n",
    "from hyperopt import hp, tpe, fmin, Trials, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "from helper.load_dataset import load_bace_classification\n",
    "from tabulate import tabulate\n",
    "from helper.preprocess import split_train_valid_test, generate_graph_dataset\n",
    "from helper.trainer import fit_model, evaluate_test, final_fit_model, final_evaluate\n",
    "from helper.graphfeat import StructureEncoderV4\n",
    "from helper.cal_metrics import classification_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718e7",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052bd65",
   "metadata": {},
   "source": [
    "##### Classification task\n",
    "\n",
    "- Encoder: Structure Encoder V1, 2, 3, 4\n",
    "\n",
    "- You can also modified number of evaluation to tune hyperparams (line 222)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c325c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bace = load_bace_classification()\n",
    "train, valid, test = split_train_valid_test(bace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937ccc1",
   "metadata": {},
   "source": [
    "#### Graph Convolutional NN, undirected graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e52efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [03:27<00:00, 103.52s/trial, best loss: -0.8044692737430168]\n",
      "\n",
      "Best parameters: {'activation': 'elu', 'batch_size': 256, 'dropout_rate': 0.3621452515715548, 'epochs': 200, 'hidden_channels': 96, 'learning_rate': 0.005412417256916041, 'linear_hidden_1': 48, 'linear_hidden_2': 12, 'num_layers': 5, 'num_linear_layers': 2, 'patience': 15, 'pooling': 'mean', 'use_edge_weight': False}\n",
      "\n",
      "Training final model for 125 epochs...\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8229  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8229  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.8126  | 0.7733 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.8355  | 0.8442 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8250  | 0.8104 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.8573  | 0.8286 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.7856  | 0.7927 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9045  | 0.8826 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.8786  | 0.8324 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "encoder = StructureEncoderV4(directed=False) # directed = False mean undirected graph\n",
    "class GraphConvClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_node_features,\n",
    "            hidden_channels=64,\n",
    "            num_layers=3,\n",
    "            dropout_rate=0.2,\n",
    "            pooling='max',\n",
    "            use_edge_weight=True,\n",
    "            num_linear_layers = 2,\n",
    "            linear_hidden_1=32,\n",
    "            linear_hidden_2=16,\n",
    "            activation='relu'\n",
    "    ):\n",
    "        super(GraphConvClassifier, self).__init__()\n",
    "        self.use_edge_weight = use_edge_weight\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.pooling = pooling\n",
    "        self.num_linear_layers=num_linear_layers\n",
    "        self.activation = activation\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GraphConv(num_node_features, hidden_channels, bias=True))\n",
    "        for _ in range(num_layers-1):\n",
    "            self.convs.append(GraphConv(hidden_channels, hidden_channels, bias=True))\n",
    "        \n",
    "        self.linears = nn.ModuleList()\n",
    "        if num_linear_layers == 1:\n",
    "            self.linears.append(nn.Linear(hidden_channels, 1))\n",
    "        elif num_linear_layers == 2:\n",
    "            self.linears.append(nn.Linear(hidden_channels, linear_hidden_1))\n",
    "            self.linears.append(nn.Linear(linear_hidden_1, 1))\n",
    "        else:\n",
    "            self.linears.append(nn.Linear(hidden_channels, linear_hidden_1))\n",
    "            self.linears.append(nn.Linear(linear_hidden_1, linear_hidden_2))\n",
    "            self.linears.append(nn.Linear(linear_hidden_2, 1))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        use_ew = edge_weight if self.use_edge_weight else None\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index, edge_weight=use_ew)\n",
    "            x = self._activation(x)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        if self.pooling == 'max':\n",
    "            x = global_max_pool(x, batch)\n",
    "\n",
    "        elif self.pooling == 'mean':\n",
    "            x = global_mean_pool(x, batch)\n",
    "        else:\n",
    "            x = global_add_pool(x, batch)\n",
    "\n",
    "        for i, lin in enumerate(self.linears[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.linears[-1](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'gelu':\n",
    "            return F.gelu(x)\n",
    "        elif self.activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        elif self.activation == 'selu':\n",
    "            return F.selu(x)\n",
    "        else:\n",
    "            return F.relu(x)\n",
    "    \n",
    "def gcn_objective(\n",
    "        params,\n",
    "        train_dataset,\n",
    "        valid_dataset,\n",
    "        num_node_features\n",
    "):\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model = GraphConvClassifier(\n",
    "        num_node_features=num_node_features,\n",
    "        hidden_channels=params['hidden_channels'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        pooling=params['pooling'],\n",
    "        use_edge_weight=params['use_edge_weight'],\n",
    "        num_linear_layers=params['num_linear_layers'],\n",
    "        linear_hidden_1=params['linear_hidden_1'],\n",
    "        linear_hidden_2=params['linear_hidden_2'],\n",
    "        activation=params['activation']\n",
    "    )\n",
    "\n",
    "    history = fit_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        epochs=params['epochs'],\n",
    "        lr=params['learning_rate'],\n",
    "        patience=params['patience'],\n",
    "        task='classification',\n",
    "        use_edge_weight=params['use_edge_weight']\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_test(model, valid_loader, task='classification')\n",
    "    f1 = metrics['f1']\n",
    "    return {\n",
    "        'loss': -f1,\n",
    "        'status': 'ok',\n",
    "        'best_num_epoch': len(history)\n",
    "    }\n",
    "\n",
    "gcn_search_space = {\n",
    "    'hidden_channels': scope.int(hp.quniform('hidden_channels', 32, 128, 16)),\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 2, 5, 1)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'batch_size': scope.int(hp.quniform('batch_size', 32, 256, 32)),\n",
    "    'num_linear_layers': scope.int(hp.quniform('num_linear_layers', 1, 3, 1)),\n",
    "    'linear_hidden_1': scope.int(hp.quniform('linear_hidden_1', 16, 64, 8)),\n",
    "    'linear_hidden_2': scope.int(hp.quniform('linear_hidden_2', 8, 32, 4)),\n",
    "    'pooling': hp.choice('pooling', ['max', 'mean']),\n",
    "    'use_edge_weight': hp.choice('use_edge_weight', [True, False]),\n",
    "    'activation': hp.choice('activation', ['relu', 'selu', 'elu', 'gelu']),\n",
    "    'epochs': 200,\n",
    "    'patience': 15\n",
    "}\n",
    "\n",
    "\n",
    "def run_gcn_tuning(train_data, valid_data, test_data, encoder, max_evals=100):\n",
    "    \n",
    "    # Generate graph datasets\n",
    "    \n",
    "    train_dataset = generate_graph_dataset(train_data, 'SMILES', 'Class', encoder=encoder)\n",
    "    valid_dataset = generate_graph_dataset(valid_data, 'SMILES', 'Class', encoder=encoder)\n",
    "    test_dataset = generate_graph_dataset(test_data, 'SMILES', 'Class', encoder=encoder)\n",
    "    \n",
    "    # Run hyperparameter optimization\n",
    "    num_node_features = train_dataset.num_node_features\n",
    "    objective_fn = lambda params: gcn_objective(\n",
    "        params, \n",
    "        train_dataset,\n",
    "        valid_dataset,\n",
    "        num_node_features\n",
    "    )\n",
    "    \n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn=objective_fn,\n",
    "        space=gcn_search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials\n",
    "    )\n",
    "    \n",
    "    best_params = space_eval(gcn_search_space, best_params)\n",
    "    print(f\"\\nBest parameters: {best_params}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    best_model = GraphConvClassifier(\n",
    "        num_node_features=num_node_features,\n",
    "        hidden_channels=best_params['hidden_channels'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout_rate=best_params['dropout_rate'],\n",
    "        pooling=best_params['pooling'],\n",
    "        use_edge_weight=best_params['use_edge_weight'],\n",
    "        num_linear_layers=best_params['num_linear_layers'],\n",
    "        linear_hidden_1=best_params['linear_hidden_1'],\n",
    "        linear_hidden_2=best_params['linear_hidden_2'],\n",
    "        activation=best_params['activation']\n",
    "    )\n",
    "    \n",
    "    merge_data = pd.concat([train_data, valid_data], ignore_index=True)\n",
    "    merge_dataset = generate_graph_dataset(merge_data, 'SMILES', 'Class', encoder=encoder)\n",
    "\n",
    "    # Create data loaders with best batch size\n",
    "    merge_loader = DataLoader(\n",
    "        merge_dataset, \n",
    "        batch_size=best_params['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=best_params['batch_size'], \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Get best epoch from trials\n",
    "    best_trial = trials.best_trial\n",
    "    best_num_epochs = best_trial['result']['best_num_epoch']\n",
    "    \n",
    "    print(f\"\\nTraining final model for {best_num_epochs} epochs...\")\n",
    "    history = final_fit_model(\n",
    "        best_model,\n",
    "        merge_loader,\n",
    "        epochs=best_num_epochs,\n",
    "        lr=best_params['learning_rate'],\n",
    "        task='classification',\n",
    "        use_edge_weight=best_params['use_edge_weight']\n",
    "    )\n",
    "    \n",
    "    train_stats = final_evaluate(best_model, merge_loader, task='classification')\n",
    "    test_stats = final_evaluate(best_model, test_loader, task='classification')\n",
    "\n",
    "    return train_stats, test_stats\n",
    "\n",
    "train_stats, test_stats = run_gcn_tuning(train, valid, test, encoder, max_evals=2)\n",
    "\n",
    "train_metrics = classification_metrics(train_stats['y_true'], train_stats['y_pred'], train_stats['y_scores'])\n",
    "test_metrics = classification_metrics(test_stats['y_true'], test_stats['y_pred'], test_stats['y_scores'])\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "# print('ANN Classifier results:')\n",
    "# print(f'Best params: {best_ann_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_gnn.txt', 'w') as file:\n",
    "    file.write(f'BACE classfication\\n')\n",
    "    file.write('ANN Classifier results:\\n')\n",
    "    # file.write(f'Best params: {best_ann_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc867163",
   "metadata": {},
   "source": [
    "### Graph Convolutional NN, directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a869617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:42<00:00, 81.27s/trial, best loss: -0.8] \n",
      "\n",
      "Best parameters: {'activation': 'elu', 'batch_size': 96, 'dropout_rate': 0.4013863889499434, 'epochs': 200, 'hidden_channels': 80, 'learning_rate': 0.00040067493926090945, 'linear_hidden_1': 48, 'linear_hidden_2': 32, 'num_layers': 5, 'num_linear_layers': 2, 'patience': 15, 'pooling': 'mean', 'use_edge_weight': False}\n",
      "\n",
      "Training final model for 198 epochs...\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8310  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8310  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.8394  | 0.8000 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.8208  | 0.8182 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8313  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.8507  | 0.8108 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.8077  | 0.8077 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9115  | 0.8381 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.8829  | 0.8009 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "encoder = StructureEncoderV4(directed=True) # directed = False mean undirected graph\n",
    "\n",
    "class GraphConvClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_node_features,\n",
    "            hidden_channels=64,\n",
    "            num_layers=3,\n",
    "            dropout_rate=0.2,\n",
    "            pooling='max',\n",
    "            use_edge_weight=True,\n",
    "            num_linear_layers = 2,\n",
    "            linear_hidden_1=32,\n",
    "            linear_hidden_2=16,\n",
    "            activation='relu'\n",
    "    ):\n",
    "        super(GraphConvClassifier, self).__init__()\n",
    "        self.use_edge_weight = use_edge_weight\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.pooling = pooling\n",
    "        self.num_linear_layers=num_linear_layers\n",
    "        self.activation = activation\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GraphConv(num_node_features, hidden_channels, bias=True))\n",
    "        for _ in range(num_layers-1):\n",
    "            self.convs.append(GraphConv(hidden_channels, hidden_channels, bias=True))\n",
    "        \n",
    "        self.linears = nn.ModuleList()\n",
    "        if num_linear_layers == 1:\n",
    "            self.linears.append(nn.Linear(hidden_channels, 1))\n",
    "        elif num_linear_layers == 2:\n",
    "            self.linears.append(nn.Linear(hidden_channels, linear_hidden_1))\n",
    "            self.linears.append(nn.Linear(linear_hidden_1, 1))\n",
    "        else:\n",
    "            self.linears.append(nn.Linear(hidden_channels, linear_hidden_1))\n",
    "            self.linears.append(nn.Linear(linear_hidden_1, linear_hidden_2))\n",
    "            self.linears.append(nn.Linear(linear_hidden_2, 1))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        use_ew = edge_weight if self.use_edge_weight else None\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index, edge_weight=use_ew)\n",
    "            x = self._activation(x)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        if self.pooling == 'max':\n",
    "            x = global_max_pool(x, batch)\n",
    "\n",
    "        elif self.pooling == 'mean':\n",
    "            x = global_mean_pool(x, batch)\n",
    "        else:\n",
    "            x = global_add_pool(x, batch)\n",
    "\n",
    "        for i, lin in enumerate(self.linears[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.linears[-1](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'gelu':\n",
    "            return F.gelu(x)\n",
    "        elif self.activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        elif self.activation == 'selu':\n",
    "            return F.selu(x)\n",
    "        else:\n",
    "            return F.relu(x)\n",
    "    \n",
    "def gcn_objective(\n",
    "        params,\n",
    "        train_dataset,\n",
    "        valid_dataset,\n",
    "        num_node_features\n",
    "):\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model = GraphConvClassifier(\n",
    "        num_node_features=num_node_features,\n",
    "        hidden_channels=params['hidden_channels'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        pooling=params['pooling'],\n",
    "        use_edge_weight=params['use_edge_weight'],\n",
    "        num_linear_layers=params['num_linear_layers'],\n",
    "        linear_hidden_1=params['linear_hidden_1'],\n",
    "        linear_hidden_2=params['linear_hidden_2'],\n",
    "        activation=params['activation']\n",
    "    )\n",
    "\n",
    "    history = fit_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        epochs=params['epochs'],\n",
    "        lr=params['learning_rate'],\n",
    "        patience=params['patience'],\n",
    "        task='classification'\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_test(model, valid_loader, task='classification')\n",
    "    f1 = metrics['f1']\n",
    "    return {\n",
    "        'loss': -f1,\n",
    "        'status': 'ok',\n",
    "        'best_num_epoch': len(history)\n",
    "    }\n",
    "\n",
    "gcn_search_space = {\n",
    "    'hidden_channels': scope.int(hp.quniform('hidden_channels', 32, 128, 16)),\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 2, 5, 1)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'batch_size': scope.int(hp.quniform('batch_size', 32, 256, 32)),\n",
    "    'num_linear_layers': scope.int(hp.quniform('num_linear_layers', 1, 3, 1)),\n",
    "    'linear_hidden_1': scope.int(hp.quniform('linear_hidden_1', 16, 64, 8)),\n",
    "    'linear_hidden_2': scope.int(hp.quniform('linear_hidden_2', 8, 32, 4)),\n",
    "    'pooling': hp.choice('pooling', ['max', 'mean']),\n",
    "    'use_edge_weight': hp.choice('use_edge_weight', [True, False]),\n",
    "    'activation': hp.choice('activation', ['relu', 'selu', 'elu', 'gelu']),\n",
    "    'epochs': 200,\n",
    "    'patience': 15\n",
    "}\n",
    "\n",
    "\n",
    "def run_gcn_tuning(train_data, valid_data, test_data, encoder, max_evals=100):\n",
    "    \n",
    "    # Generate graph datasets\n",
    "    \n",
    "    train_dataset = generate_graph_dataset(train_data, 'SMILES', 'Class', encoder=encoder)\n",
    "    valid_dataset = generate_graph_dataset(valid_data, 'SMILES', 'Class', encoder=encoder)\n",
    "    test_dataset = generate_graph_dataset(test_data, 'SMILES', 'Class', encoder=encoder)\n",
    "    \n",
    "    # Run hyperparameter optimization\n",
    "    num_node_features = train_dataset.num_node_features\n",
    "    objective_fn = lambda params: gcn_objective(\n",
    "        params, \n",
    "        train_dataset,\n",
    "        valid_dataset,\n",
    "        num_node_features\n",
    "    )\n",
    "    \n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn=objective_fn,\n",
    "        space=gcn_search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials\n",
    "    )\n",
    "    \n",
    "    best_params = space_eval(gcn_search_space, best_params)\n",
    "    print(f\"\\nBest parameters: {best_params}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    best_model = GraphConvClassifier(\n",
    "        num_node_features=num_node_features,\n",
    "        hidden_channels=best_params['hidden_channels'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout_rate=best_params['dropout_rate'],\n",
    "        pooling=best_params['pooling'],\n",
    "        use_edge_weight=best_params['use_edge_weight'],\n",
    "        num_linear_layers=best_params['num_linear_layers'],\n",
    "        linear_hidden_1=best_params['linear_hidden_1'],\n",
    "        linear_hidden_2=best_params['linear_hidden_2'],\n",
    "        activation=best_params['activation']\n",
    "    )\n",
    "    \n",
    "    merge_data = pd.concat([train_data, valid_data], ignore_index=True)\n",
    "    merge_dataset = generate_graph_dataset(merge_data, 'SMILES', 'Class', encoder=encoder)\n",
    "\n",
    "    # Create data loaders with best batch size\n",
    "    merge_loader = DataLoader(\n",
    "        merge_dataset, \n",
    "        batch_size=best_params['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=best_params['batch_size'], \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Get best epoch from trials\n",
    "    best_trial = trials.best_trial\n",
    "    best_num_epochs = best_trial['result']['best_num_epoch']\n",
    "    \n",
    "    print(f\"\\nTraining final model for {best_num_epochs} epochs...\")\n",
    "    history = final_fit_model(\n",
    "        best_model,\n",
    "        merge_loader,\n",
    "        epochs=best_num_epochs,\n",
    "        lr=best_params['learning_rate'],\n",
    "        task='classification'\n",
    "    )\n",
    "    \n",
    "    train_stats = final_evaluate(best_model, merge_loader, task='classification')\n",
    "    test_stats = final_evaluate(best_model, test_loader, task='classification')\n",
    "\n",
    "    return train_stats, test_stats\n",
    "\n",
    "train_stats, test_stats = run_gcn_tuning(train, valid, test, encoder, max_evals=2)\n",
    "\n",
    "train_metrics = classification_metrics(train_stats['y_true'], train_stats['y_pred'], train_stats['y_scores'])\n",
    "test_metrics = classification_metrics(test_stats['y_true'], test_stats['y_pred'], test_stats['y_scores'])\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "# print('ANN Classifier results:')\n",
    "# print(f'Best params: {best_ann_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_gnn.txt', 'w') as file:\n",
    "    file.write(f'BACE classfication\\n')\n",
    "    file.write('ANN Classifier results:\\n')\n",
    "    # file.write(f'Best params: {best_ann_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
